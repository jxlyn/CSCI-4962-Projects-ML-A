{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWS2JpxLEOEGL1HiyuQUHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jxlyn/CSCI-4962-Projects-ML-AI/blob/main/Homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1(50 points): We discussed how we can formulate RL problems as an MDP. Describe any\n",
        "real-world application that can be formulated as an MDP. Describe the state space, action\n",
        "space, transition model, and rewards for that problem. You do not need to be precise in the\n",
        "description of the transition model and reward (no formula is needed). Qualitative description\n",
        "is enough.**"
      ],
      "metadata": {
        "id": "2GAZaSISh0yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A real-world application that can be formulated as an MDP is a gold finder machine. The gold finder machine will detect whether or not the machine catch the gold or not, then it will use its claw to bring up the gold. The states for this machine are nothing, gold, and no gold. Nothing means there's nothing available at current position. Gold means there are gold at current postion. No gold means there's no gold at current postion. The actions for this machine are use claw to bring the item up at current position, or move to other position. The transition model of this machine is when the machine catched a lots of gold or big gold at current position, it has higher probability to search again on current postion. If it catched nothing or items that are not gold, it will have higher probability to move to other position. The reward is the net worth of the gold based on the gold size. If there's nothing, the reward will be -1 because it takes time and money to operate the machine. If there's something that is not gold, the reward is 0."
      ],
      "metadata": {
        "id": "j3T6yrozh6Eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2(50 points): RL is used in various sectors - Healthcare, recommender systems and trading\n",
        "are a few of those. Pick one of the three areas. Explain one of the problems in any of these\n",
        "domains that can be more effectively solved by reinforcement learning. Find an open-source\n",
        "project (if any) that has addressed this problem. Explain this project in detail.**"
      ],
      "metadata": {
        "id": "BHDfIaSrq52X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement learning can effectively solve the healthcare problem. For example, the article \"Using Reinforcement Learning to Identify High-risk States and Treatments in Healthcare\" by Mehdi Fatemi, Taylor Killian, and  Marzyeh Ghassemi addressed that doctors can use reinforcement learning models to identify high-risk treatments for a patient. The model that the authors proposed can be expressed as a Markov Decision Process. The states for this model are rescue and dead-end states. The rescue state means that patient is fully recovered. The dead-end state means that patient is dead. In most cases, the probability of either state will not be 1, it's always somewhere in between. Thus, doctors must know that a patient is highly likely to enter a dead-end state. The action is to select a treatment. The transition is to observe the patient's state after the patient receives the treatment. If the patient doesn't survive, the agent will get a penalty. The reward signal will be -1. On the other hand, if the patient recovers, the agent will get a reward. The reward signal will be +1. According to the article, it states that \" â€“ğ‘„âˆ—ğ·(ğ‘ ,ğ‘) corresponds to the minimum probability of a future negative outcome if treatment ğ‘ is selected at state ğ‘ . Equivalently, 1+ğ‘„âˆ—ğ·(ğ‘ ,ğ‘) corresponds to the maximum hope of a positive outcome.\" This means we can analyze the Qd to see whether or not a patient will survive. Also, we can see that if the probability of treatment selection can be higher than ğ‘„âˆ—ğ‘…(ğ‘ ,ğ‘), the patient is guaranteed to remain in a rescue state when possible. We can record the state data at different times, such as 4 hours, 8 hours, and 12 hours ... to see whether or not a treatment will lead to a dead end. In the article, the authors used this model in detecting the dead-end treatment for sepsis. They used the dataset from Beth Israel Deaconess Medical Center in Boston, Massachusetts. They concluded that more than 12 percent of treatments given to non-surviving patients could be detrimental 24 hours before death. This information is important for doctors because it can potentially help doctors provide patients with better care, such as changing the treatment. As we can see, this model is extremely useful for doctors because it will prevent them to prescribe unsuitable medicine to patients. It will increase the survival rate of the patients. "
      ],
      "metadata": {
        "id": "lP40vcRCrMn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.microsoft.com/en-us/research/blog/using-reinforcement-learning-to-identify-high-risk-states-and-treatments-in-healthcare/#:~:text=Reinforcement%20learning%20for%20healthcare&text=In%20healthcare%2C%20clinicians%20base%20their,treatment%2C%20and%20the%20process%20repeats."
      ],
      "metadata": {
        "id": "xTnGiN-b9sU2"
      }
    }
  ]
}